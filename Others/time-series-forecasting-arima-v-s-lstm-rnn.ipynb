{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This kernel allows for data exploration & diagnostics before fitting models. Two forecasting techniques are explored viz... LSTM - RNN & SARIMA. The hyperparameters are auto-tuned using gridsearchCV in case of LSTM & for loops in case of SARIMA. There is further scope to improve the forecast by removing/treating outliers.\n\n# load and plot dataset\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\nfrom importlib import reload\nfrom numpy import array\nimport numpy\n\nimport sys\nfrom imp import reload\nif sys.version[0] == '2':\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")\n\n#Importing data\ndf = read_csv('../input/norway_new_car_sales_by_model.csv',encoding='cp1252')\n\n#Printing head\ndf.head()\ndf.tail()\n\n# Summarizing the dataset\n#How many rows the dataset have\ndf['Quantity'].count()\n\nfrom pandas import Series, DataFrame\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\ndf['Date'] = pd.to_datetime(df.assign(Day=1).loc[:, ['Year','Month','Day']])\n#retain only date & quantity\ndf = df[['Date','Quantity']]\ndf = df.groupby(['Date']).sum()\ndf.head()\n\n# Plotting data\ndf['Quantity'].plot(color='red')\nplt.xlabel('Date')\nplt.ylabel('Quantity')\nplt.title('Sales of cars')\nplt.show()\n\n# The plot created from running the below shows a relatively strong positive correlation between observations and their lag1 values\nfrom pandas.plotting import lag_plot\nlag_plot(df)\nplt.title('Plot of lag values 1')\npyplot.show()\n\n\n# Autocorrelation plot for the time series\nfrom pandas.plotting import autocorrelation_plot\nfrom pandas.core import datetools\nfrom statsmodels.graphics.tsaplots import plot_acf\nplot_acf(df, lags=36)\npyplot.show\n\n# Partial Autocorrelation plot for the time series\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(df, lags=50)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Functions that will be used during the model build phase\n# This segment does not produce any output\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom pandas import Series\nfrom pandas import concat\n\n# convert time series into supervised learning problem\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(len(n_in), 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, len(n_out)):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n\n#data = df\n#series_to_supervised(df, n_in=3, n_out=5, dropnan=True)\n\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# transform series into train and test sets for supervised learning\ndef prepare_data(series, n_test, n_lag, n_seq):\n    # extract raw values\n    raw_values = series.values\n    # transform data to be stationary\n    diff_series = difference(raw_values, 1)\n    diff_values = diff_series.values\n    diff_values = diff_values.reshape(len(diff_values), 1)\n    # rescale values to -1, 1\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaled_values = scaler.fit_transform(diff_values)\n    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n    # transform into supervised learning problem X, y\n    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n    supervised_values = supervised.values\n    # split into train and test sets\n    train, test = supervised_values[0:-len(n_test)], supervised_values[-len(n_test):]\n    return scaler, train, test\n\n# fit an LSTM network to training data\ndef fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n    # reshape training into [samples, timesteps, features]\n    X, y = train[:, 0:n_lag], train[:, n_lag:]\n    X = X.reshape(X.shape[0], 1, X.shape[1])\n    # design network\n    model = Sequential()\n    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(y.shape[1]))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    # fit network\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n        model.reset_states()\n    return model\n\n# make one forecast with an LSTM,\ndef forecast_lstm(model, X, n_batch):\n    # reshape input pattern to [samples, timesteps, features]\n    X = X.reshape(1, 1, len(X))\n    # make forecast\n    forecast = model.predict(X, batch_size=n_batch)\n    # convert to array\n    return forecast\n \n# make all forecasts & append them\ndef make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n    forecasts = list()\n    for i in range(len(test)):\n        X, y = test[i, 0:n_lag], test[i, n_lag:]\n        # make forecast\n        forecast = forecast_lstm(model, X, n_batch)\n        # store the forecast\n        forecasts.append(forecast)\n    return forecasts\n \n    \n# invert differenced forecast\ndef inverse_difference(last_ob, forecast):\n    # invert first forecast\n    inverted = list()\n    inverted.append(forecast[0] + last_ob)\n    # propagate difference forecast using inverted first value\n    for i in range(1, len(forecast)):\n        inverted.append(forecast[i] + inverted[i-1])\n    return inverted\n \n# inverse data transform on forecasts\ndef inverse_transform(series, forecasts, scaler, n_test):\n    inverted = list()\n    for i in range(len(forecasts)):\n        # create array from forecast\n        forecast = array(forecasts[i])\n        forecast = forecast.reshape(1, n_seq)\n        # invert scaling\n        inv_scale = scaler.inverse_transform(forecast)\n        inv_scale = inv_scale[0, :]\n        # invert differencing\n        index = len(series) - n_test + i - 1\n        last_ob = series.values[index]\n        inv_diff = inverse_difference(last_ob, inv_scale)\n        # store\n        inverted.append(inv_diff)\n    return inverted\n \n# evaluate the RMSE for each forecast time step\ndef evaluate_forecasts(test, forecasts, n_lag, n_seq):\n    for i in range(n_seq):\n        actual = [row[i] for row in test]\n        predicted = [forecast[i] for forecast in forecasts]\n        rmse = sqrt(mean_squared_error(actual, predicted))\n        print('t+%d RMSE: %f' % ((i+1), rmse))\n\n       \n# plot the forecasts in the context of the original dataset\ndef plot_forecasts(series, forecasts, n_test):\n    # plot the entire dataset in blue\n    pyplot.plot(series.values)\n    # plot the forecasts in red\n    for i in range(len(forecasts)):\n        off_s = len(series) - n_test + i - 1\n        off_e = off_s + len(forecasts[i]) + 1\n        xaxis = [x for x in range(off_s, off_e)]\n        yaxis = [series.values[off_s]] + forecasts[i]\n        pyplot.plot(xaxis, yaxis, color='red')\n    # show the plot\n    pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f1894b7d7e1fa6c561c3d12d922764ed4787d2b"},"cell_type":"code","source":"# Automated tuning of hyperparameters using Grid Search. \n\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# fix random seed for reproducibility\nseed = 5\nnumpy.random.seed(seed)\n\n# fit an LSTM network to training data\ndef fit_lstm_tuning(n_test = 28, n_lag = 1, n_batch = 1, n_neurons = 1, activation = 'sigmoid', optimizer = 'adam'):\n    # design network\n    model = Sequential()\n    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, 1, n_lag), activation = activation, stateful=True))\n    model.add(Dense(y.shape[1]))\n    #model.add(Activation('sigmoid'))\n    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# define the grid search parameters\nn_lag = [1]\n#n_lag = [*map(int, n_lag)]\nn_seq = [1]\n#n_epochs = [1500, 2000, 2500, 3000]\nn_batch = [1]\nn_neurons = [1,2,3]\nactivation = ['softmax', 'relu', 'tanh', 'sigmoid']\noptimizer = ['SGD', 'RMSprop','Adam']\n# configure\nseries = df\nn_test = [26]\n\n#series = series.values.tolist()\nscaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\nX_input, y_dependent = train[:, 0:len(n_lag)], train[:, len(n_lag):]\nX, y = X_input, y_dependent\nX = X.reshape(X.shape[0], 1, X.shape[1])\n\n# create model\nmodel = KerasClassifier(build_fn=fit_lstm_tuning, epochs = 1500, batch_size = 1, verbose=0)\n\nparam_grid = dict(n_batch = n_batch, n_neurons = n_neurons)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\ngrid_result = grid.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bd34e78805ed44afaaa95fff40592977e8bbd96","collapsed":true},"cell_type":"code","source":"# ARIMA model for time series forecasting. This section provides a comparison of forecasts obtained using ARIMA.\n\nimport warnings\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n#Visualizing Time-series Data\ndf.plot(figsize=(15, 6))\nplt.show()\n\n#time-series seasonal decomposition\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 11, 9\n\ndecomposition = sm.tsa.seasonal_decompose(df, model='additive')\nfig = decomposition.plot()\nplt.show()\n\n# Parameter Selection for the ARIMA Time Series Model\n\n#Define the p, d and q parameters to take any value between 0 and 2\np = range(0, 3)\nd = range(0, 3)\nq = range(0, 1)\n\n#Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n#Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n\n# All combinations of parameters are used & best set of parameters with Minimum AIC is selected\nAIC_list = pd.DataFrame({}, columns=['param','param_seasonal','AIC'])\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(df,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            results = mod.fit()\n\n            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n            temp = pd.DataFrame([[ param ,  param_seasonal , results.aic ]], columns=['param','param_seasonal','AIC'])\n            AIC_list = AIC_list.append( temp, ignore_index=True)  # DataFrame append 는 일반 list append 와 다르게 이렇게 지정해주어야한다.\n            del temp\n\n        except:\n            continue\n\n\nm = np.nanmin(AIC_list['AIC'].values) # Find minimum value in AIC\nl = AIC_list['AIC'].tolist().index(m) # Find index number for lowest AIC\nMin_AIC_list = AIC_list.iloc[l,:]\n\n# Fitting ARIMA model wtih best parameters obtained using Minimum AIC\nmod = sm.tsa.statespace.SARIMAX(df,\n                                order=Min_AIC_list['param'],\n                                seasonal_order=Min_AIC_list['param_seasonal'],\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\n\nprint(\"### Min_AIC_list ### \\n{}\".format(Min_AIC_list))\n\nprint(results.summary().tables[1])\n\nresults.plot_diagnostics(figsize=(15, 12))\nplt.show()\n\n# Validating Forecasts\npred_dynamic = results.get_prediction(start=pd.to_datetime('2016-01-01'), dynamic=True, full_results=True)\npred_dynamic_ci = pred_dynamic.conf_int()\n\nax = df['2007':].plot(label='observed')\npred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n\nax.fill_between(pred_dynamic_ci.index,\n                pred_dynamic_ci.iloc[:, 0],\n                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n\nax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2016-01-01'), df.index[-1],\n                 alpha=.1, zorder=-1)\n\nax.set_xlabel('Date')\nax.set_ylabel('Car Sales')\n\nplt.legend()\nplt.show()\n\n#Extract the predicted and true values of our time series\ny_forecasted = pred_dynamic.predicted_mean\ny_truth = df['2016-01-01':]\n\n#Compute the mean square error\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))            \n            \n         \n# Producing and Visualizing Forecasts\n#Get forecast 'x' steps ahead in future\npred_uc = results.get_forecast(steps=24)\n\n#Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()\n\n#plot the time series and forecasts of its future values\nax = df.plot(label='observed', figsize=(20, 15))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Car Sales')\n\nplt.legend()\nplt.show()\n\n# I began with order of moving average (value of q) ranging from 1 to 3. However, any moving average order is causing the accuracy to dip. I removed the order of moving average (q) as the p-value was not significant & was also impacting other order of AR (p-value to be insignificant for oder of AR). This forecast is after removing order of moving average. It can be noticed that there is an outlier (Feb 2015) which is being captured in the model & impacting the forecast. As a next step this outlier can be treated & forecast exercise run again.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}